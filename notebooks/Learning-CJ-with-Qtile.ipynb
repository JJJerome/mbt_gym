{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # <-- Path to the DRL4AMM repo\n",
    "\n",
    "from DRL4AMM.gym.AvellanedaStoikovEnvironment import AvellanedaStoikovEnvironment\n",
    "from DRL4AMM.rewards.RewardFunctions import PnL, CJ_criterion\n",
    "\n",
    "from DRL4AMM.tile_coding.tile_coding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea58483",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "n_steps = 1000\n",
    "arrival_rate = 50.0\n",
    "reward_function = CJ_criterion()\n",
    "timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "env_params = dict(terminal_time=terminal_time, n_steps=n_steps, arrival_rate=arrival_rate, reward_function=reward_function)\n",
    "env = AvellanedaStoikovEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Action Space {}\".format(env.action_space))\n",
    "#print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create our tilings\n",
    "\n",
    "feature_ranges = [[lb,ub] for lb,ub in zip(list(env.observation_space.low), list(env.observation_space.high))]  \n",
    "number_tilings = 1\n",
    "bins = [[10, 10, 10, 10]]  # each tiling has a 10*10 grid\n",
    "offsets = [[0, 0, 0, 0]]\n",
    "tilings = create_tilings(feature_ranges, number_tilings, bins, offsets)\n",
    "print(tilings.shape)  # # of tilings X features X bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create our tilings\n",
    "\n",
    "#feature_ranges = [[lb,ub] for lb,ub in zip(list(env.observation_space.low), list(env.observation_space.high))]  \n",
    "#number_tilings = 3\n",
    "#bins = [[10, 10, 10, 10], [10, 10, 10, 10], [10, 10, 10, 10]]  # each tiling has a 10*10 grid\n",
    "#offsets = [[0, 0, 0, 0], [10, 1000, 10, 0.1], [20, 2000, 20, 0.2]]\n",
    "#tilings = create_tilings(feature_ranges, number_tilings, bins, offsets)\n",
    "#print(tilings.shape)  # # of tilings X features X bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce61f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_space():\n",
    "        \n",
    "    from itertools import product\n",
    "\n",
    "    base1 = np.arange(0,1,0.05)\n",
    "    base2 = np.arange(0,1,0.05)\n",
    "    action_space = np.array(list(product(*[base1, base2])),dtype='float32')\n",
    "    action_space = list(action_space)\n",
    "    \n",
    "    return [tuple(a) for a in action_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = get_action_space()\n",
    "q = QValueFunction(tilings, action_space, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N_STEPS = 1000\n",
    "\n",
    "new_state = env.reset()\n",
    "\n",
    "# arbitrary first action\n",
    "action = action_space[0]\n",
    "\n",
    "keep_rewards = []\n",
    "\n",
    "for i in tqdm(range(N_STEPS)):\n",
    "    \n",
    "    # take the new action\n",
    "    new = env.step(action)\n",
    "    \n",
    "    # new state and collected reward\n",
    "    old_state = new_state\n",
    "    new_state = new[0]\n",
    "    reward = new[1]\n",
    "    \n",
    "    # keep reward \n",
    "    keep_rewards.append(reward)\n",
    "    \n",
    "    # create Q target\n",
    "    target = q.get_target(reward, list(new_state))\n",
    "    \n",
    "    q.update(list(state),action,target)\n",
    "    \n",
    "    action = q.eps_greedy(new_state)\n",
    "    \n",
    "    if i % 100 == 0:    \n",
    "        rewards = pd.Series(keep_rewards)\n",
    "        plt.plot(rewards.rolling(100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(keep_rewards)\n",
    "plt.plot(rewards.rolling(100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6b809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
