{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # <-- Path to the DRL4AMM repo\n",
    "\n",
    "\n",
    "from DRL4AMM.rewards.RewardFunctions import PnL, CJ_criterion\n",
    "\n",
    "from DRL4AMM.tile_coding.tile_coding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150db7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DRL4AMM.gym.MMAtTouchEnvironment import MMAtTouchEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea58483",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "n_steps = 100\n",
    "arrival_rate = 50.0\n",
    "reward_function = CJ_criterion()\n",
    "timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "\n",
    "#env_params = dict(terminal_time=terminal_time, n_steps=n_steps, arrival_rate=arrival_rate, reward_function=reward_function)\n",
    "#env = AvellanedaStoikovEnvironment(**env_params)\n",
    "\n",
    "env =  MMAtTouchEnvironment()\n",
    "\n",
    "MMAtTouchEnvironment\n",
    "\n",
    "from DRL4AMM.gym.wrappers import *\n",
    "env = ReduceStateSizeWrapper(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock price, Cash, Inventory, Time left\n",
    "\n",
    "#print(\"Action Space {}\".format(env.action_space))\n",
    "#print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create our tilings\n",
    "\n",
    "n_features = len(list(env.observation_space.low))\n",
    "feature_ranges = [[lb,ub] for lb,ub in zip(list(env.observation_space.low), list(env.observation_space.high))]  \n",
    "number_tilings = 1\n",
    "n_bins = 10\n",
    "bins = [[n_bins] * n_features]  \n",
    "offsets = [[0, 0, 0, 0]]\n",
    "tilings = create_tilings(feature_ranges, number_tilings, bins, offsets)\n",
    "print(tilings.shape)  # # of tilings X features X bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create our tilings\n",
    "\n",
    "#feature_ranges = [[lb,ub] for lb,ub in zip(list(env.observation_space.low), list(env.observation_space.high))]  \n",
    "#number_tilings = 3\n",
    "#bins = [[10, 10, 10, 10], [10, 10, 10, 10], [10, 10, 10, 10]]  # each tiling has a 10*10*10*10 grid\n",
    "#offsets = [[0, 0, 0, 0], [10, 1000, 10, 0.1], [20, 2000, 20, 0.2]]\n",
    "#tilings = create_tilings(feature_ranges, number_tilings, bins, offsets)\n",
    "#print(tilings.shape)  # # of tilings X features X bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02275f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_space():\n",
    "        \n",
    "    from itertools import product\n",
    "\n",
    "    base1 = np.arange(0,0.25,0.005)\n",
    "    base2 = np.arange(0,0.25,0.005)\n",
    "    action_space = np.array(list(product(*[base1, base2])),dtype='float32')\n",
    "    action_space = list(action_space)\n",
    "    \n",
    "    return [tuple(a) for a in action_space]\n",
    "\n",
    "def get_action_space_touch():\n",
    "        \n",
    "    return [(0,0), (0,1), (1,0), (1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_space = get_action_space()\n",
    "action_space = get_action_space_touch()\n",
    "\n",
    "q = QValueFunction(tilings, action_space, lr = 0.01, gamma = 0.99, eps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f58c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N_STEPS = 1000000\n",
    "\n",
    "new_state = list(env.reset())\n",
    "\n",
    "# arbitrary first action\n",
    "action = action_space[0]\n",
    "\n",
    "keep_rewards = []\n",
    "keep_actions = []\n",
    "keep_episodic_rewards = []\n",
    "\n",
    "episodic_reward = 0\n",
    "\n",
    "q.eps = 1\n",
    "\n",
    "for i in tqdm(range(N_STEPS)):\n",
    "    \n",
    "    # take the new action\n",
    "    new = env.step(action)\n",
    "    \n",
    "    # new state and collected reward\n",
    "    old_state = new_state\n",
    "    new_state = list(new[0])\n",
    "    reward = new[1]\n",
    "    done = new[2]\n",
    "\n",
    "    # increment episodic reward\n",
    "    episodic_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        keep_episodic_rewards.append(episodic_reward)\n",
    "        #print(\"Episodic reward\", episodic_reward)\n",
    "        \n",
    "        # reset episodic reward\n",
    "        episodic_reward = 0\n",
    "        new_state = list(env.reset())\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # create Q target\n",
    "        target = q.get_target(reward, new_state)\n",
    "    \n",
    "        # perform Q update\n",
    "        q.update(old_state, action, target)\n",
    "    \n",
    "        # select new action based on new state\n",
    "        action = q.eps_greedy(new_state)\n",
    "    \n",
    "            \n",
    "    # shrink eps\n",
    "    q.eps *= 0.999\n",
    "    \n",
    "    # keep reward and action\n",
    "    keep_rewards.append(reward)\n",
    "    keep_actions.append(action)\n",
    "    \n",
    "    if i % 100 == 0:    \n",
    "        #print(\"Iteration: %s, eps: %s\" % (i, q.eps))\n",
    "        #rewards = pd.Series(keep_rewards)\n",
    "        #print(rewards.rolling(100).mean().tail(1))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = pd.Series(keep_rewards)\n",
    "rewards.rolling(500).mean().plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_df = pd.DataFrame(keep_actions)\n",
    "actions_df.columns = ['action 1', 'action 2']\n",
    "actions_df.plot(subplots=True, layout=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards = pd.Series(keep_episodic_rewards)\n",
    "episodic_rewards.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic_rewards.rolling(500).mean().plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_series = pd.Series(keep_actions)\n",
    "tmp = actions_series.value_counts()\n",
    "tmp[tmp>0]\n",
    "#tmp[tmp>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc2e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
