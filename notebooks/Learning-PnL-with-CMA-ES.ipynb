{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorboard\n",
    "import torch as th\n",
    "from scipy import stats\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import stable_baselines3\n",
    "\n",
    "from stochastic.processes.continuous import BrownianMotion, GeometricBrownianMotion, BesselProcess, BrownianBridge, BrownianMeander\n",
    "from stochastic.processes.diffusion import ConstantElasticityVarianceProcess\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # <-- Path to the main repo\n",
    "\n",
    "from main.agents.Agent import Agent\n",
    "from main.agents.AvellanedaStoikovAgent import AvellanedaStoikovAgent\n",
    "from main.agents.BaselineAgents import RandomAgent, FixedSpreadAgent\n",
    "from main.agents.SBAgent import SBAgent\n",
    "from main.gym.ModelBasedEnvironment import ModelBasedEnvironment\n",
    "from main.gym.models import *\n",
    "from main.gym.wrappers import *\n",
    "from main.gym.AvellanedaStoikovEnvironment import AvellanedaStoikovEnvironment\n",
    "from main.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from main.rewards.RewardFunctions import InventoryAdjustedPnL\n",
    "from main.gym.helpers.plotting import plot_stable_baselines_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed539841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63602a",
   "metadata": {},
   "source": [
    "### Investigating the expected rewards of fixed strategies by sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "n_steps = 200\n",
    "timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "env_params = dict(terminal_time=terminal_time, n_steps=n_steps, max_half_spread = 1)\n",
    "as_env = AvellanedaStoikovEnvironment(**env_params)\n",
    "reduced_env = ReduceStateSizeWrapper(as_env)\n",
    "\n",
    "gym.envs.register(id=\"as-env-v0\", entry_point=\"__main__:AvellanedaStoikovEnvironment\", kwargs=env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 100\n",
    "def fixed_strat_neg_reward(half_spread:float, env:gym.Env=reduced_env):\n",
    "    total_rewards = 0\n",
    "    for _ in range(N_EPISODES):\n",
    "        _,_,episode_rewards = generate_trajectory(env,FixedSpreadAgent(half_spread))\n",
    "        total_rewards+= sum(episode_rewards)\n",
    "    return -total_rewards/N_EPISODES    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_spread_rewards = [-fixed_strat_neg_reward(hs) for hs in np.linspace(0,2,200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a19bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,2,200), fixed_spread_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62e5d6",
   "metadata": {},
   "source": [
    "## Finding the best fixed strategy with CMA-ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef19103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma\n",
    "\n",
    "x0 = [1]\n",
    "sigma0 = 1\n",
    "es = cma.CMAEvolutionStrategy(2 * [x0], sigma0, {'CMA_on':0})\n",
    "es.optimize(lambda x:fixed_strat_neg_reward(x[0])) # on the fly 2-D -> 1-D wrapper\n",
    "es.logger.plot(xsemilog=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.result_pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_agent = AvellanedaStoikovAgent(risk_aversion=0)\n",
    "as_action = as_agent.get_action([0,0,0,0])[0]\n",
    "cma_action = es.result.xbest[0]\n",
    "print(f\"Optimal strategy is {as_action, as_action}\\nCMA-ES strategy is {cma_action,cma_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Error is {round(abs(cma_action-as_action)/as_action*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
