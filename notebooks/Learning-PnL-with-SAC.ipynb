{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorboard\n",
    "import torch as th\n",
    "from scipy import stats\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import stable_baselines3\n",
    "\n",
    "from stochastic.processes.continuous import BrownianMotion, GeometricBrownianMotion, BesselProcess, BrownianBridge, BrownianMeander\n",
    "from stochastic.processes.diffusion import ConstantElasticityVarianceProcess\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # <-- Path to the main repo\n",
    "\n",
    "from main.agents.Agent import Agent\n",
    "from main.agents.AvellanedaStoikovAgent import AvellanedaStoikovAgent\n",
    "from main.agents.BaselineAgents import RandomAgent, FixedSpreadAgent\n",
    "from main.agents.SBAgent import SBAgent\n",
    "from main.gym.ModelBasedEnvironment import ModelBasedEnvironment\n",
    "from main.gym.models import *\n",
    "from main.gym.wrappers import *\n",
    "from main.gym.AvellanedaStoikovEnvironment import AvellanedaStoikovEnvironment\n",
    "from main.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from main.rewards.RewardFunctions import PnL#InventoryAdjustedPnL\n",
    "from main.gym.helpers.plotting import plot_stable_baselines_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed539841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63602a",
   "metadata": {},
   "source": [
    "### Learning Inventory-neutral behaviour with SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a348770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a linearly decreasing learning rate function\n",
    "def linear_schedule(initial_value):\n",
    "    def func(progress):\n",
    "        return progress * initial_value\n",
    "\n",
    "    return func\n",
    "schedule = linear_schedule(0.00003) # Here, we use the default SB value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a141c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = \"./tensorboard/SAC-learning-AS-pnl/\"\n",
    "best_model_path = \"./SB_models/PPO-best-PnL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "n_steps = 1000\n",
    "arrival_rate = 50.0\n",
    "timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "env_params = dict(terminal_time=terminal_time, n_steps=n_steps, arrival_rate=arrival_rate)\n",
    "as_env = AvellanedaStoikovEnvironment(**env_params)\n",
    "reduced_env = ReduceStateSizeWrapper(as_env)\n",
    "\n",
    "n_envs = 6\n",
    "gym.envs.register(id=\"as-env-v0\", entry_point=\"__main__:AvellanedaStoikovEnvironment\", kwargs=env_params)\n",
    "vec_env = make_vec_env(env_id=\"as-env-v0\", n_envs=n_envs, wrapper_class=ReduceStateSizeWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[64, 64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_params = {\"policy\":'MlpPolicy', \"env\": vec_env, \"verbose\":1, \n",
    "              \"policy_kwargs\":policy_kwargs, \n",
    "              \"tensorboard_log\":tensorboard_logdir,\n",
    "              \"batch_size\": 2048, \"learning_rate\": schedule} #256 before (batch size)\n",
    "callback_params = dict(eval_env=reduced_env, n_eval_episodes = 2048, #200 before  (n_eval_episodes)\n",
    "                       best_model_save_path = best_model_path, \n",
    "                       deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8547ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EvalCallback(**callback_params)\n",
    "model = SAC(**sac_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learning_rate = linear_schedule(0.00001*0.01)\n",
    "#model.batch_size = 256*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd043157",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = 1_000_000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1131f",
   "metadata": {},
   "source": [
    "### Plotting the agent's action against their inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c18131",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SAC.load(best_model_path+\"/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cccb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([best_model.predict([0,0.5])[0][0] for _ in range(10000)])\n",
    "plt.hist(actions, bins = 100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = [-3,-2,-1,0,1,2,3]\n",
    "[best_model.predict([inventory,0. ], deterministic=True)[0] for inventory in inventories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9867393",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean action: {np.median(actions)}, Median action: {np.mean(actions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fed3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SAC.load(best_model_path+\"/best_model\")\n",
    "#best_model = model\n",
    "inventories = np.arange(-10,11,1)#[-3,-2,-1,0,1,2,3]\n",
    "as_agent = AvellanedaStoikovAgent(risk_aversion=0)\n",
    "as_actions = np.array([as_agent.get_action([100,0,inventory,0.0]) for inventory in inventories])\n",
    "actions = np.array([best_model.predict([inventory,0.6 ], deterministic=True)[0] for inventory in inventories])\n",
    "plt.plot(inventories, actions[:,0], label=\"bid\")\n",
    "plt.plot(inventories, actions[:,1], label=\"ask\")\n",
    "plt.plot(inventories, as_actions[:,0], label=\"optimal AS action\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "actions = {}\n",
    "for inventory in inventories:\n",
    "    actions[inventory] = np.array([best_model.predict([inventory,ts], deterministic=True)[0] for ts in timestamps])    \n",
    "    plt.plot(timestamps, actions[inventory][:,0], label = \"bid\"+str(inventory))\n",
    "    plt.plot(timestamps, actions[inventory][:,1], label = \"ask\"+str(inventory))\n",
    "as_actions = np.array([as_agent.get_action([0,0,0,0]) for ts in timestamps])\n",
    "plt.plot(timestamps, as_actions[:,0], label=\"AS-action\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510779ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array([best_model.predict([inventory,3])[0][0] for _ in range(10000)])\n",
    "plt.hist(actions, bins = 100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305ab0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
