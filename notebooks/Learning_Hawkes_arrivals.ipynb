{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b918608",
   "metadata": {},
   "source": [
    "### Import external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0a2c9",
   "metadata": {},
   "source": [
    "### Add mbt-gym to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.BaselineAgents import CarteaJaimungalMmAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from mbt_gym.gym.StableBaselinesTradingEnvironment import StableBaselinesTradingEnvironment\n",
    "from mbt_gym.gym.TradingEnvironment import TradingEnvironment\n",
    "from mbt_gym.gym.ModelDynamics import LimitOrderModelDynamics\n",
    "from mbt_gym.gym.wrappers import *\n",
    "from mbt_gym.rewards.RewardFunctions import CjCriterion, CjMmCriterion\n",
    "from mbt_gym.stochastic_processes.midprice_models import *\n",
    "from mbt_gym.stochastic_processes.fill_probability_models import *\n",
    "from mbt_gym.stochastic_processes.arrival_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d65b0",
   "metadata": {},
   "source": [
    "### Add parameters for limit order market making environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41296b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "arrival_rate = 10.0\n",
    "n_steps = int(10 * terminal_time * arrival_rate)\n",
    "phi = 0.5\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cj_env_Poisson(num_trajectories:int = 1):    \n",
    "    fill_exponent = 1\n",
    "    sigma = 0.1\n",
    "    initial_inventory = (-4,5)\n",
    "    initial_price = 100\n",
    "    step_size = terminal_time/n_steps\n",
    "    max_depth = 5\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionMidpriceModel(volatility=sigma, \n",
    "                                                                   terminal_time=terminal_time, \n",
    "                                                                   step_size=step_size, \n",
    "                                                                   initial_price=initial_price, \n",
    "                                                                   num_trajectories=num_trajectories)\n",
    "    arrival_model = PoissonArrivalModel(intensity=np.array([arrival_rate, arrival_rate]), step_size=step_size)\n",
    "    fill_probability_model = ExponentialFillFunction(fill_exponent=fill_exponent, \n",
    "                                                                       step_size=step_size, \n",
    "                                                                       num_trajectories=num_trajectories)\n",
    "    LOtrader = LimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                num_trajectories = num_trajectories, max_depth = max_depth)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      reward_function = CjMmCriterion(phi, alpha),\n",
    "                      max_inventory=n_steps,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa98a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cj_env_Hawkes(num_trajectories:int = 1):    \n",
    "    fill_exponent = 1\n",
    "    sigma = 0.1\n",
    "    initial_inventory = (-4,5)\n",
    "    initial_price = 100\n",
    "    max_depth = 5\n",
    "    step_size = terminal_time/n_steps\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionMidpriceModel(volatility=sigma, \n",
    "                                                                   terminal_time=terminal_time, \n",
    "                                                                   step_size=step_size, \n",
    "                                                                   initial_price=initial_price, \n",
    "                                                                   num_trajectories=num_trajectories)\n",
    "    arrival_model = HawkesArrivalModel(num_trajectories=num_trajectories, step_size=step_size)\n",
    "    fill_probability_model = ExponentialFillFunction(fill_exponent=fill_exponent, \n",
    "                                                                       step_size=step_size, \n",
    "                                                                       num_trajectories=num_trajectories)\n",
    "    LOtrader = LimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                num_trajectories = num_trajectories, max_depth = max_depth)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      reward_function = CjMmCriterion(phi, alpha),\n",
    "                      max_inventory=n_steps,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29022e",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "num_trajectories = 1000\n",
    "env = ReduceStateSizeWrapper(get_cj_env_Hawkes(num_trajectories), [1,2,4,5])\n",
    "sb_env = StableBaselinesTradingEnvironment(trading_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d657c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_agent = CarteaJaimungalMmAgent(env=get_cj_env_Poisson(num_trajectories), max_inventory = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f837dc9",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Monitor sb_env\n",
    "sb_env = VecMonitor(sb_env)\n",
    "# Add directory for tensorboard logging \n",
    "tensorboard_logdir = \"./tensorboard/PPO-learning-Hawkes/\"\n",
    "best_model_path = \"./SB_models/PPO-best-Hawkes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106df91",
   "metadata": {},
   "source": [
    "### Define PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7065586",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[dict(pi=[64, 64], vf=[128, 128])])\n",
    "PPO_params = {\"policy\":'MlpPolicy', \"env\": sb_env, \"verbose\":1, \n",
    "              \"policy_kwargs\":policy_kwargs, \n",
    "              \"tensorboard_log\":tensorboard_logdir,\n",
    "              \"batch_size\": int(n_steps * num_trajectories / 20), \n",
    "              \"n_steps\": int(n_steps)} #256 before (batch size)\n",
    "callback_params = dict(eval_env=sb_env, n_eval_episodes = 2048, \n",
    "                       eval_freq = 200,#200 before  (n_eval_episodes)\n",
    "                       best_model_save_path = best_model_path, \n",
    "                       deterministic=True)\n",
    "\n",
    "callback = EvalCallback(**callback_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(**PPO_params, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01707612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = 1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74b6cd",
   "metadata": {},
   "source": [
    "## Comparing the learnt policy to the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.SbAgent import SbAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SbAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-3,4,1)\n",
    "bid_actions = {}\n",
    "ask_actions = {}\n",
    "intensities=[5,10,20]\n",
    "for intensity in intensities:\n",
    "    bid_actions[intensity] = []\n",
    "    ask_actions[intensity] = []\n",
    "    for inventory in inventories:\n",
    "        bid_action, ask_action = np.reshape(model.predict([inventory,0.5, intensity, intensity], deterministic=True)[0], 2)    \n",
    "        bid_actions[intensity].append(bid_action)\n",
    "        ask_actions[intensity].append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cj_agent.get_action(np.array([[0,inventory,0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a344c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Cartea Jaimungal action\n",
    "cj_bid_actions = []\n",
    "cj_ask_actions = []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = cj_agent.get_action(np.array([[0,inventory,0.5]]))[0,:].reshape(-1)\n",
    "    cj_bid_actions.append(bid_action)\n",
    "    cj_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"k\", \"r\", \"b\"]\n",
    "\n",
    "for i, intensity in enumerate(intensities):\n",
    "    plt.plot(inventories, bid_actions[intensity], label = f\"bid - lambda = {intensity}\", color = colors[i])\n",
    "    plt.plot(inventories, ask_actions[intensity], label = f\"ask - lambda = {intensity}\", color = colors[i], linestyle = \"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57074bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.arange(0,1 + 0.01, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27489a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bid_actions = {}\n",
    "ask_actions = {}\n",
    "\n",
    "# loop over intensities? (currently just using 10 below)\n",
    "intensities = [5,10,15,20]\n",
    "\n",
    "for intensity in intensities:\n",
    "\n",
    "    bid_actions[intensity] = {}\n",
    "    ask_actions[intensity] = {}\n",
    "    \n",
    "    for inventory in inventories:\n",
    "        bid_actions[intensity][inventory] = []\n",
    "        ask_actions[intensity][inventory] = []\n",
    "        for timestamp in timestamps:\n",
    "            state = np.array([[inventory, timestamp, intensity, intensity]])\n",
    "            bid_action, ask_action = agent.get_action(state)[0]\n",
    "            bid_actions[intensity][inventory].append(bid_action)\n",
    "            ask_actions[intensity][inventory].append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.get_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb129cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(state, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dcb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(intensities), figsize=(18, 5))\n",
    "\n",
    "for i,intensity in enumerate(intensities):\n",
    "    \n",
    "    ax = axs[i]\n",
    "\n",
    "    for inventory in inventories:\n",
    "        ax.plot(timestamps, bid_actions[intensity][inventory], label=f\"bid: q = {inventory}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Bid action\")  \n",
    "    ax.set_title('intensity: %d' % intensity)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59213c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inventory in inventories:\n",
    "    plt.plot(timestamps, ask_actions[inventory], label=f\"ask: q = {inventory}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = PPO.load(\"trained_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18558350",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b2796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
