{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b097fbb8",
   "metadata": {},
   "source": [
    "# Learning to make a market with mbt_gym and Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b918608",
   "metadata": {},
   "source": [
    "### Import external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0a2c9",
   "metadata": {},
   "source": [
    "### Add mbt-gym to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387934ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb89dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.BaselineAgents import CarteaJaimungalMmAgent\n",
    "from mbt_gym.gym.helpers.generate_trajectory import generate_trajectory\n",
    "from mbt_gym.gym.StableBaselinesTradingEnvironment import StableBaselinesTradingEnvironment\n",
    "from mbt_gym.gym.TradingEnvironment import TradingEnvironment\n",
    "from mbt_gym.gym.wrappers import *\n",
    "from mbt_gym.rewards.RewardFunctions import PnL, CjMmCriterion\n",
    "from mbt_gym.stochastic_processes.midprice_models import BrownianMotionMidpriceModel\n",
    "from mbt_gym.stochastic_processes.arrival_models import PoissonArrivalModel\n",
    "from mbt_gym.stochastic_processes.fill_probability_models import ExponentialFillFunction\n",
    "from mbt_gym.gym.ModelDynamics import LimitOrderModelDynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d65b0",
   "metadata": {},
   "source": [
    "### Create market making environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_time = 1.0\n",
    "arrival_rate = 10.0\n",
    "n_steps = int(10 * terminal_time * arrival_rate)\n",
    "phi = 0.5\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11432746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cj_env(num_trajectories:int = 1):\n",
    "    fill_exponent = 1\n",
    "    sigma = 0.1\n",
    "    initial_inventory = (-4,5)\n",
    "    initial_price = 100\n",
    "    step_size = 1/n_steps\n",
    "    timestamps = np.linspace(0, terminal_time, n_steps + 1)\n",
    "    midprice_model = BrownianMotionMidpriceModel(volatility=sigma, step_size=1/n_steps,\n",
    "                                                 num_trajectories=num_trajectories)\n",
    "    arrival_model = PoissonArrivalModel(intensity=np.array([arrival_rate, arrival_rate]), \n",
    "                                        step_size=1/n_steps, \n",
    "                                        num_trajectories=num_trajectories)\n",
    "    fill_probability_model = ExponentialFillFunction(fill_exponent=fill_exponent, \n",
    "                                                     step_size=1/n_steps,\n",
    "                                                     num_trajectories=num_trajectories)\n",
    "    LOtrader = LimitOrderModelDynamics(midprice_model = midprice_model, arrival_model = arrival_model, \n",
    "                                fill_probability_model = fill_probability_model,\n",
    "                                num_trajectories = num_trajectories)\n",
    "    reward_function = CjMmCriterion(per_step_inventory_aversion = phi, terminal_inventory_aversion = alpha)\n",
    "    env_params = dict(terminal_time=terminal_time, \n",
    "                      n_steps=n_steps,\n",
    "                      initial_inventory = initial_inventory,\n",
    "                      model_dynamics = LOtrader,\n",
    "                      max_inventory=n_steps,\n",
    "                      normalise_action_space = False,\n",
    "                      normalise_observation_space = False,\n",
    "                      reward_function = reward_function,\n",
    "                      num_trajectories=num_trajectories)\n",
    "    return TradingEnvironment(**env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajectories = 1000\n",
    "env = ReduceStateSizeWrapper(get_cj_env(num_trajectories))\n",
    "sb_env = StableBaselinesTradingEnvironment(trading_env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f837dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor sb_env\n",
    "sb_env = VecMonitor(sb_env)\n",
    "# Add directory for tensorboard logging and best model\n",
    "tensorboard_logdir = \"./tensorboard/PPO-learning-CJ/\"\n",
    "best_model_path = \"./SB_models/PPO-best-CJ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106df91",
   "metadata": {},
   "source": [
    "### Define PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=[dict(pi=[256, 256], vf=[256, 256])])\n",
    "PPO_params = {\"policy\":'MlpPolicy', \"env\": sb_env, \"verbose\":1, \n",
    "              \"policy_kwargs\":policy_kwargs, \n",
    "              \"tensorboard_log\":tensorboard_logdir,\n",
    "              \"n_epochs\":3,\n",
    "              \"batch_size\": int(n_steps * num_trajectories / 10), \n",
    "              \"n_steps\": int(n_steps)}\n",
    "callback_params = dict(eval_env=sb_env, n_eval_episodes = 2048, #200 before  (n_eval_episodes)\n",
    "                       best_model_save_path = best_model_path, \n",
    "                       deterministic=True)\n",
    "\n",
    "callback = EvalCallback(**callback_params)\n",
    "model = PPO(**PPO_params, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01707612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = 10_000_000)  # Increase number of training timesteps according to computing resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74b6cd",
   "metadata": {},
   "source": [
    "## Comparing the learnt policy to the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mbt_gym.agents.SbAgent import SbAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent = SbAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventories = np.arange(-3,4,1)\n",
    "bid_actions = []\n",
    "ask_actions = []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = ppo_agent.get_action(np.array([[inventory,0.5]])).reshape(-1)\n",
    "    bid_actions.append(bid_action)\n",
    "    ask_actions.append(ask_action)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3df21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo_agent.get_action(np.array([[inventory,0.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_agent = CarteaJaimungalMmAgent(env=get_cj_env())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a344c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Cartea Jaimungal action\n",
    "cj_bid_actions = []\n",
    "cj_ask_actions = []\n",
    "for inventory in inventories:\n",
    "    bid_action, ask_action = cj_agent.get_action(np.array([[0,inventory,0.5]])).reshape(-1)\n",
    "    cj_bid_actions.append(bid_action)\n",
    "    cj_ask_actions.append(ask_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inventories, bid_actions, label = \"bid\", color = \"k\")\n",
    "plt.plot(inventories, ask_actions, label = \"ask\", color = \"r\")\n",
    "plt.plot(inventories, cj_bid_actions, label = \"bid cj\", color = \"k\", linestyle = \"--\")\n",
    "plt.plot(inventories, cj_ask_actions, label = \"ask cj\", color = \"r\", linestyle = \"--\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
